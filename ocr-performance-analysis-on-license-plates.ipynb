{"cells":[{"cell_type":"markdown","metadata":{},"source":["<a href=\"https://www.kaggle.com/code/daiqing2009/ocr-performance-analysis-on-license-plates?scriptVersionId=172543186\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"]},{"cell_type":"markdown","metadata":{},"source":["# OCR Performance Analysis on LICENSE PLATES\n","This notebook replies on the magnificant job done of https://www.kaggle.com/code/aslanahmedov/automatic-number-plate-recognition \n",", which trained a fine-tuned YOLO Model for license plate location in each picutre.\n","We utilize the that in the first part License plate images Preparation as the basis of our project to analyze the OCR performance on \n","* different scales of data(i.e. license plate images)\n","* different (combination of) hardware\n"]},{"cell_type":"markdown","metadata":{},"source":["# License plate images Preparation\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Dataset Analysis\n","\n","| Name | Number of picture(approximate) | Need Crop | Crop Method | Remark|\n","|--- | --- | --- | --- | --- |\n","|number-plate-detection| 250 | Y | coordinates in XML | high-resolution pic |\n","|car-plate - detection | 500 | Y | coordinates in XML | medium-resolution pic  |\n","|indian-vehicle-dataset|1,000 | Y | coordinates in XML | low-medium resolution pic, license plate no. in XML|\n","| stanford-cars-dataset|20,000 | Y | annotation in mat | details in mat file unknown |\n","| synthetic-turkish-license-plates | 100,000 | N | N/A | synthtic crated, license plate no. as file name |\n","| us-license-plates | 4,000 | N | N/A | high-resulution front-view license plates with states name/symbol |\n","| us-license-plates | 20,000 | N | N/A | low-medium resulution front-view license plates with states name/symbol |\n","\n","Besides, there's a dataset called **license-plate-digits-classification-dataset** so enable us to train own number recognition NN if needed. "]},{"cell_type":"markdown","metadata":{},"source":["## Generate licence plates pictures from datasets\n","Since threre're plenty of **front-view license plates pictures with no need to crop** , it is  reasonable to work on those before turn to those pictures needing crop. "]},{"cell_type":"markdown","metadata":{},"source":["Here we will not only detect license plate but else extract test from it. We already done it before letâ€™s repeat it again. We will write function. First from bounding box we need to take our x,y,w,h and extract ROI. I place it at top of our functions in drawing part."]},{"cell_type":"markdown","metadata":{},"source":["# OPTICAL CHARACTER RECOGNITION(OCR) Performance Analysis\n","\n","## Baseline TESSERACT OCR\n","\n","Optical character recognition (OCR) software that is used to extract text from the image. Tesseract OCR have a python API and it is open source. Firstly, we will do installation of it. It pretty simple and depend on you OS. You can find manual and files to download for installation [here](https://guides.library.illinois.edu/c.php?g=347520&p=4121425).\n","\n","\n","### LIMITATIONS OF PYTESSERACT\n","\n","Tesseract works best when there is a clean segmentation of the foreground text from the background. In practice, it can be extremely challenging to guarantee these types of setups. There are a variety of reasons you might not get good quality output from Tesseract like if the image has noise on the background. The better the image quality (size, contrast, lightning) the better the recognition result. It requires a bit of preprocessing to improve the OCR results, images need to be scaled appropriately, have as much image contrast as possible, and the text must be horizontally aligned. Tesseract OCR is quite powerful but does have the following limitations.\n","\n","__Tesseract limitations summed in the list.__\n","<ul>\n","  <li>The OCR is not as accurate as some commercial solutions available to us.</li>\n","  <li>Doesn't do well with images affected by artifacts including partial occlusion, distorted perspective, and complex background.</li>\n","  <li>It is not capable of recognizing handwriting.</li>\n","  <li>It may find gibberish and report this as OCR output.</li>\n","  <li>If a document contains languages outside of those given in the -l LANG arguments, results may be poor.</li>  \n","  <li>It is not always good at analyzing the natural reading order of documents. For example, it may fail to recognize that a document contains two columns, and may try to join text across columns.</li>\n","  <li>Poor quality scans may produce poor quality OCR.</li>\n","  <li>It does not expose information about what font family text belongs to.</li>\n","</ul>\n","\n","\n","### EXTRACT NUMBER PLATE TEXT FROM IMAGE\n","\n","Firstly, we will load our image and convert to array. Crop our bounding box with coordinates of it. We will identify region of interest (ROI) and have look at our cropped image *Figure 15*."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:49:14.394995Z","iopub.status.busy":"2024-04-12T18:49:14.394546Z","iopub.status.idle":"2024-04-12T18:49:14.759737Z","shell.execute_reply":"2024-04-12T18:49:14.758471Z","shell.execute_reply.started":"2024-04-12T18:49:14.394962Z"},"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'cv2'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpytesseract\u001b[39;00m\n\u001b[1;32m      5\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/us-license-plates-image-classification/new plates/test/ALABAMA/3.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"]}],"source":["import cv2 \n","import pytesseract\n","\n","# Path to the image file\n","path = \"/kaggle/input/us-license-plates-image-classification/new plates/test/ALABAMA/3.jpg\"\n","img = cv2.imread(path)\n","\n","# extract the characters from the image\n","text = pytesseract.image_to_string(img)\n","print(text)"]},{"cell_type":"markdown","metadata":{},"source":["With use of tesseract, we will extract the text from the mage.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Compare the performance with Spark OCR\n","https://nlp.johnsnowlabs.com/docs/en/ocr"]},{"cell_type":"markdown","metadata":{},"source":["## Improve the performance by utilizing GPU"]},{"cell_type":"markdown","metadata":{},"source":["# Ultimate Challenge: Hand-made OCR optimized for license Plate Recognition (good to have)"]},{"cell_type":"markdown","metadata":{},"source":["# "]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":1603227,"sourceId":2780636,"sourceType":"datasetVersion"},{"datasetId":3523965,"sourceId":6159065,"sourceType":"datasetVersion"}],"dockerImageVersionId":30198,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
